@inproceedings{he2020deberta,
  title     = {Deberta: Decoding-enhanced bert with disentangled attention},
  author    = {He, Pengcheng and Liu, Xiaodong and Wang, Jianfeng and Li, Weizhu and Liu, Yelong and Zhao, Xiuying and Xiao, Xinyan and Liu, Jiawei and Lyu, Yeyun},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {156--168},
  year      = {2020}
}
@article{tunstall2022efficient,
  title   = {Efficient few-shot learning without prompts},
  author  = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
  journal = {arXiv preprint arXiv:2209.11055},
  year    = {2022}
}

@article{touvron2023llama,
  title   = {Llama 2: Open foundation and fine-tuned chat models},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}

@inproceedings{maheshwari2021scibert,
  title     = {Scibert sentence representation for citation context classification},
  author    = {Maheshwari, Himanshu and Singh, Bhavyajeet and Varma, Vasudeva},
  booktitle = {Proceedings of the Second Workshop on Scholarly Document Processing},
  pages     = {130--133},
  year      = {2021}
}

@inproceedings{kobayashi2023two,
  title     = {Two-Way Multi-Label Loss},
  author    = {Kobayashi, Takumi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {7476--7485},
  year      = {2023}
}

@article{yang2018sgm,
  title   = {SGM: sequence generation model for multi-label classification},
  author  = {Yang, Pengcheng and Sun, Xu and Li, Wei and Ma, Shuming and Wu, Wei and Wang, Houfeng},
  journal = {arXiv preprint arXiv:1806.04822},
  year    = {2018}
}

@inproceedings{ridnik2021asymmetric,
  title     = {Asymmetric loss for multi-label classification},
  author    = {Ridnik, Tal and Ben-Baruch, Emanuel and Zamir, Nadav and Noy, Asaf and Friedman, Itamar and Protter, Matan and Zelnik-Manor, Lihi},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {82--91},
  year      = {2021}
}

@article{kementchedjhieva2023exploration,
  title   = {An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text},
  author  = {Kementchedjhieva, Yova and Chalkidis, Ilias},
  journal = {arXiv preprint arXiv:2305.05627},
  year    = {2023}
}

@article{oyen2022robustness,
  title   = {Robustness to Label Noise Depends on the Shape of the Noise Distribution},
  author  = {Oyen, Diane and Kucer, Michal and Hengartner, Nicolas and Singh, Har Simrat},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {35645--35656},
  year    = {2022}
}
@incollection{kumar2020robust,
  title     = {Robust learning of multi-label classifiers under label noise},
  author    = {Kumar, Himanshu and Manwani, Naresh and Sastry, PS},
  booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
  pages     = {90--97},
  year      = {2020}
}

@article{chi2024graph,
  title   = {Graph Neural Network Based Multi-Label Hierarchical Classification for Disease Predictions in General Practice.},
  author  = {Chi, Shengqiang and Wang, Yuqing and Zhang, Ying and Zhu, Weiwei and Li, Jingsong},
  journal = {Studies in Health Technology and Informatics},
  volume  = {310},
  pages   = {725--729},
  year    = {2024}
}

@article{VanLandeghem2022a,
  author  = {Van Landeghem, Jordy and Blaschko, Matthew and Anckaert, Bertrand and Moens, Marie-Francine},
  year    = {2022},
  title   = {{Benchmarking Scalable Predictive Uncertainty in Text Classification}},
  journal = {IEEE Access},
  doi     = {10.1109/ACCESS.2022.3168734}
}

@inproceedings{VanLandeghem2023dude,
  author    = {Van Landeghem, Jordy and Tito, Rub{\`e}n and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Joziak, Pawel and Powalski, Rafal and Jurkiewicz, Dawid and Coustaty, Micka{\"e}l and Anckaert, Bertrand and Valveny, Ernest and Blaschko, Matthew and Moens, Marie-Francine and Stanis{\l}awek, Tomasz},
  year      = {2023},
  title     = {{Document Understanding Dataset and Evaluation (DUDE)}},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {19528--19540}
}

@inproceedings{ridnik2021asymmetric,
  title     = {Asymmetric loss for multi-label classification},
  author    = {Ridnik, Tal and Ben-Baruch, Emanuel and Zamir, Nadav and Noy, Asaf and Friedman, Itamar and Protter, Matan and Zelnik-Manor, Lihi},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {82--91},
  year      = {2021}
}

@article{devlin2018bert,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year      = {2018},
  month     = {June},
  title     = {{Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding}},
  journal   = {arXiv preprint arXiv:1810.04805},
  booktitle = {{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics}},
  publisher = {Association for Computational Linguistics},
  address   = {Minneapolis, Minnesota},
  pages     = {4171--4186},
  doi       = {10.18653/v1/N19-1423},
  url       = {https://www.aclweb.org/anthology/N19-1423},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\\%} (4.6{\\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}